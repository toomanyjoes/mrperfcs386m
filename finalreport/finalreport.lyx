#LyX 1.6.4 created this file. For more info see http://www.lyx.org/
\lyxformat 345
\begin_document
\begin_header
\textclass article
\begin_preamble
%% LyX 1.6.4 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.



\usepackage[letterpaper]{geometry}

\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=0.75in,rmargin=0.75in}
\end_preamble
\options twocolumn
\use_default_options false
\language english
\inputencoding latin9
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 0
\use_esint 0
\cite_engine basic
\use_bibtopic false
\paperorientation portrait
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\defskip medskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\author "" 
\end_header

\begin_body

\begin_layout Title
Edge-Based Cloud Computing as a Feasible Network Paradigm
\end_layout

\begin_layout Author
Joe Elizondo and Samuel Palmer 
\begin_inset Newline newline
\end_inset

 Department of Computer Science University of Texas at Austin
\begin_inset Newline newline
\end_inset

 Email: {elizondo, spalmer}@cs.utexas.edu 
\end_layout

\begin_layout Abstract
The term edge based cloud computing refers to a network of edge systems
 that provide the services currently provided by data center clouds.
 In this paper we present modifications to MRPerf, an existing tool used
 to simulate MapReduce in data center clouds, that enable it to simulate
 Hadoop MapReduce jobs in edge networks.
 Our results indicate that the default Hadoop scheduler does not perform
 optimally in an edge environment and should be replaced.
\end_layout

\begin_layout Section
Introduction 
\begin_inset CommandInset label
LatexCommand label
name "sec:Introduction"

\end_inset


\end_layout

\begin_layout Standard
Edge based cloud computing is a combination of two popular ideas, edge computing
 and cloud computing.
 We believe that the combination of these two ideas offers the potential
 for a high performance per dollar ratio by leveraging the enormous amount
 of unused computational power in idle home computer processors that have
 access to always-on internet connections.
 Despite its potential this type of computing paradigm is not being used
 and is only sparsely investigated.
 This paper will present the results of our research on edge based cloud
 computing and some of the benefits and challenges we discovered through
 our work.
 In order to determine the full potential of this non-existent network we
 use simulation and run MapReduce jobs that push the network beyond the
 usual peer-to-peer/grid computing type simple data transfer that these
 topologies are commonly used for.
 MapReduce operations involve a high level of coordination and communication
 between machines in addition to transferring large amounts of data.
 For our simulation environment we investigate MRPerf, a MapReduce tool
 built on top of the network simulator ns-2, which was developed by researchers
 at Virginia Tech and IBM Almaden Research Center.
 To simulate an edge based network we will change the parameters in our
 configuration to better mimic a peer-to-peer type system of nodes without
 central infrastructure.
 Each node has fewer resources and is located at a further distance from
 one another than nodes in a data center.
 We also customize our simulation settings to account for sluggish nodes
 and node churn.
 Our final report will probably have a structure similar to the following.
 Section 2 we will discuss some related work, section 3 we will present
 an overview of the tools used, section 4 we will talk about some of the
 challenges we faced in designing our simulation, section 5 we discuss our
 implementation, section 6 we will discuss some experiments and results,
 section 7 we will present some conclusions, and section 8 we will talk
 about potential future research in this area.
\end_layout

\begin_layout Section
Related Work
\begin_inset CommandInset label
LatexCommand label
name "sec:Related-Work"

\end_inset


\end_layout

\begin_layout Standard
[7 or whatever] presented LATE, a scheduling algorithm to handle heterogeneity
 in a data center environment.
 LATE attempts to schedule tasks based on the longest approximate time to
 completion.
 The approximation is based on the heterogeneity of the nodes and the current
 progress of the task.
 The work presented in [7] provides useful insights into the scheduling
 process in heterogeneous environments.
\end_layout

\begin_layout Standard
In general, work applying cloud computing concepts directly to edge networks
 is sparse and this area of research is still in a stage of early development.
 Because of this our work focused more on the feasibility of edge based
 cloud computing, MapReduce in particular, from the aspect of performance
 and in doing so we ignored many practical issues such as security and privacy.
\end_layout

\begin_layout Section
Tools
\begin_inset CommandInset label
LatexCommand label
name "sec:Tools"

\end_inset


\end_layout

\begin_layout Standard
We leveraged two existing projects for our work.
 The first is MRPerf [2][3].
 It was designed to simulate MapReduce jobs when provided with a network
 topology for a data center.
 MRPerf can measure the performance of these MapReduce jobs with a good
 amount of accuracy and deliver performance information to the user.
 
\end_layout

\begin_layout Standard
We considered the use of a topology generator such as GT-ITM 
\backslash
cite:gtitm or BRITE 
\backslash
cite:brite but decided that we would obtain more realistic results if we
 used information about the actual Internet instead of generating router
 topologies.
 We decided to use the data collected from the Rocketfuel
\backslash
cite:Spring02 project which mapped a significant portion of the Internet.
 These data provided us with a realistic topology for edge based simulations.
\end_layout

\begin_layout Subsection
MRPerf
\begin_inset CommandInset label
LatexCommand label
name "sub:MRPerf"

\end_inset


\end_layout

\begin_layout Standard
We chose to use MRPerf because of its impressive implementation of MapReduce
 and the accuracy of its simulation results.
 Our research involves testing an edge computing network with demanding
 highly coordinated applications.
 MapReduce is a good tool for testing this because its design requires participa
ting machines to break up a computationally intensive job into separate
 tasks, requiring complex scheduling algorithms, process the tasks, and
 then merge them back together upon completion.
 MRPerf was build on top of two popular packages, ns-2[6] and hadoop[7].
 MRPerf merges MapReduce and network simulation to achieve a seamless simulation
 environment.
 MRPerf claims to have the ability to simulate a wide range of network topologie
s and also has the ability to evaluate the performance of MapReduce operations
 on that network.
 It was originally designed for use in a data center type infrastructure.
 With that in consideration, MRPerf claims to predict simulation performance
 within 5.22% of actual measurements for map and 12.83% for reduce for a double
 rack cluster with 16 to 128 cores[3].
 Although MRPerf was designed to simulate MapReduce in multi-rack data center
 with many nodes per rack and high bandwidth single hop links connecting
 them together it allows some flexibility since it was built on top of the
 ns-2 network simulator.
 This gave us the capability to simulate any topology that ns-2 supports
 using MRPerf.
 ns-2 has already proven to be capable of simulating a large variety of
 networks including networks based on grid computing and peer to peer type
 topologies [10] [11] that our network closely resembles.
\end_layout

\begin_layout Subsection
Rocketfuel
\end_layout

\begin_layout Standard
We wanted to run our simulation on a fairly large network to get an accurate
 idea of performance when route scheduling and link latency are taken into
 consideration.
 Rocketfuel was a project completed in 2003 at the University of Washington
 that mapped ISP topologies using traceroutes from 800 different vantage
 points.
 Their maps cover ISPs in the United States, Europe, Australia, and India.
 We used the link latencies discovered by the Rocketfuel project to as input
 to our simulations.
 This allows us to use a more accurate representation of a real edge based
 network for our simulations.
\end_layout

\begin_layout Section
Simulation Design
\end_layout

\begin_layout Standard
In addition to using an accurate edge topology we also needed to modify
 some of the parameters used in MRPerf to simulate MapReduce tasks.
 MRPerf simulates MapReduce tasks based on three classes of parameters:
 cluster parameters, configuration parameters, and framework parameters.
 The rest of this section is dedicated to describing these parameters as
 well as modifications that could be made to improve performance in edge
 networks.
 Section 
\backslash
section:Implementation describes the modifications that we actually made
 to MRPerf for our evaluations.
 
\end_layout

\begin_layout Subsection
Cluster parameters 
\end_layout

\begin_layout Standard
MRPerf is designed to work with a data center using racks which consist
 of many nodes that are close to each other on the network, this means that
 a packet many times is just one hop away from its destination.
 These racks of nodes are connected together to form a cluster.
 In an edge computing network there is no concept of a centralized cluster
 of computational power.
 Edge computing networks use PCs that are connected to the Internet via
 a LAN through a router or gateway.
 This means that a packet's destination is almost always several network
 hops away.
 The racks that make up a cluster have many compute nodes each of which
 have several processors whereas a PC in an edge network has anywhere from
 one to four processors.
 Another difference in the cluster parameters is related to node heterogeneity.
 A data center setup has homogeneous nodes, whereas in an edge network nodes
 have a high level of heterogeneity.
 The interconnect topologies also play a role in the cluster parameters.
 The inter/intra-rack topology of a data center is much less complex than
 the Internet.
\end_layout

\begin_layout Subsection
Configuration parameters
\end_layout

\begin_layout Standard
Configuration parameters are related to the MapReduce jobs themselves.
 They adjust settings such as data chuck size, data replication, tasks per
 node, and tasks in each job.
 All of these settings are optimized for a data center by default.
 Some of the changes we have to make will depend upon the optimal configuration
 of an edge computing network.
 Whether data chunk size is large or small depends upon whether an edge
 computing system performs better when I/O is optimized or when parallelism
 in increased.
 The cost of data replication is clearly higher in an edge computing network
 than in a data center, however our experiments attempt to discover the
 optimal amount of replication among nodes.
\end_layout

\begin_layout Subsection
Framework parameters
\end_layout

\begin_layout Standard
Framework parameters deal with data placement and scheduling algorithms.
 The distance of nodes relative the machine requesting jobs needs to be
 considered when partitioning data across available resources.
 In a data center with many compute nodes the wrong choice of data placement
 can affect performance to a degree, but is not nearly as costly as the
 wrong choice when compute nodes could potentially be in a different country.
 MRPerf compares three types of data locality, node-local, rack-local, rack-remo
te.
 Our simulations use the rack-remote algorithm in MRPerf, which was the
 slowest for all of the data center evaluations [3].
 The scheduling algorithm can also be optimized for edge networks to account
 for node heterogeneity, bandwidth capacity, and round trip time.
 As discussed above the proper balance of data chunk size, data placement,
 and task scheduling are important so that the system does not suffer huge
 performance losses transferring large amounts of data over relatively slow
 links to a non-optimal number of nodes.
\end_layout

\begin_layout Subsection
Limitations
\end_layout

\begin_layout Standard
There are a couple of limitations that are inherent when working with MRPerf
 that are worth discussing.
 These limitations are related to disk storage and simulation of node reliabilit
y.
 The former works to our advantage in this case.
 MRPerf only allows for one single storage device per node.
 In a data center this might be an unusual constraint however since the
 average PC that constitutes our edge computing network will have only a
 single storage disk this should not present a problem in our evaluation.
 The early release of MRPerf available does not currently support realistic
 node failures or lagging nodes.
 Ideally, we would have been able to simulate lagging nodes and failed nodes
 in our evaluation.
 However, without an accurate mechanism to simulate these cases we leave
 this for future work.
\end_layout

\begin_layout Section
Implementation
\end_layout

\begin_layout Standard
The goal of our implementation was to discover if the MapReduce framework
 is feasible in an edge network.
 The implementation used in our simulations addressed some but not all of
 the issues discussed in Section 
\backslash
ref:simulationDesign.
 We discuss the issues that have not been addressed in Section 
\backslash
ref:FutureWork.
\end_layout

\begin_layout Subsection
MRPerf Input and Pre-Processing Overview
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename mrperf_arch.png
	width 7.8cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
(a) shows the original MRPerf architecture.
 The shaded components in (b) were changed or added for the edge based MapReduce
 simulations.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Figure 
\backslash
figure:MRPerfArch (a) shows the original MRPerf architecute.
 The topology, data layout, and job specifications are provided as input
 in the form of XML and TCL code.
 The topology is provided in XML format which is parsed by a Python script.
 This script outputs the necessary TCL code for use in ns-2 and an XML data
 layout file which is read directly by the MRPerf module integrated into
 ns-2.
 In addition to the XML input which specifies the topology the TCL input
 contains the MapReduce job specification such as the number of map and
 reduce slots available for each node.
 These variables are used directly by the simulator.
\end_layout

\begin_layout Standard
Figure 
\backslash
figure:MRPerfArch (b) highlights the changes and additions to the MRPerf
 architecture that were required to simulate the edge based networks.
 The basic idea behind our modifications was to create "racks" each containing
 one node.
 These one node "racks" could then use the remote rack scheduling algorithm
 to receive data and tasks across the network.
 The following discussion outlines the modifications made to each part of
 the MRPerf architecture in order to realize this idea.
 
\end_layout

\begin_layout Standard

\emph on
Topology and Topology Reader.

\emph default
 The original construction of MRPerf accounted for the CPU speed, amount
 of memory, and I/O speed but forced all nodes participating in a computation
 to be homogeneous.
 We changed the semantics of the topology reader to allow a different host
 configuration for each rack.
 We then employed the use of a script to generate sets of end hosts which
 we attached to the routers.
 Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:hostSpecs"

\end_inset

 summarizes the range of specifications that we used in our evaluation.
 In addition to including a range of host specifications we modified the
 Topology Reader to randomly choose the bandwidth of the last hop from the
 Internet router to the host based on a normal distribution.
 In our evaluation we set the mean of the distribution at 16 Mbps with a
 standard deviation of 8 Mbps.
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="9" columns="2">
<features>
<column alignment="center" valignment="middle" width="2.6cm">
<column alignment="center" valignment="middle" width="5cm">
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
CPU speed (GHz) 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1.5, 1.6, 1.8, 2.0, 2.3, 2.4, 2.5, 3.0, 3.2
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Number of CPUs
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1, 2
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Number of cores per CPU
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1, 2, 4
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Number of disks
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1, 2
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Disk capacity (GB)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
40, 60, 80, 100, 120, 160, 180, 200, 250, 300, 400, 450, 500, 550, 600,
 650, 700, 750, 800, 850, 900, 950, 1000
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Disk read bandwidth (MB/s)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
250, 260, 270, 280
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Disk write bandwidth (MB/s)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
60, 65, 70, 75
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
NIC capacity
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
100Mbps, 1Gbps
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Memory capacity (GB)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.5, 1, 2, 3, 4, 5, 6
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Summary of ranges used in host specification
\begin_inset CommandInset label
LatexCommand label
name "tab:hostSpecs"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\emph on
Data Layout and Layout Reader.

\emph default
 The data layout components of MRPerf control size of data chunks and the
 initial scheduling of data chunks to individual disks.
 The semantic modification we made in the topology portion of MRPerf forced
 us to also modify the initial chunk distribution.
 
\end_layout

\begin_layout Standard

\emph on
Router Topology.

\emph default
 The original Topology Reader component of MRPerf generated a complete data
 center topology script for use in ns-2 based on a specification provided
 in the form of an XML document.
 The results for the experiments with hosts distributed across the mapped
 Internet are presented in Figure TCL code that is interpreted by ns-2.
 For our simulations we directly converted the topologies provided by the
 Rocketfuel results into TCL code and simply made the Topology Reader output
 a call to this code.
 
\end_layout

\begin_layout Section
Experiments and Results
\begin_inset CommandInset label
LatexCommand label
name "sec:Experiments-and-Results"

\end_inset


\end_layout

\begin_layout Standard
We conducted experiments in which we varied several aspects of the simulation
 including the number of nodes, chunk size, and the number of mapper and
 reducer slots per node.
 The results presented in this section are the average over five experiments
 with the same parameters but different seeds for our pseudo-random number
 generator.
 All of the hosts in the edge experiments All statistically insignificant
 data points have been removed.
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align left
\begin_inset Graphics
	filename results/results_dataCenterChunkSizeBoth.png
	width 8.7cm

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
Results for 50 node topologies in a double rack data center configuration
 (a) and a random configuration distributed across the entire mapped portion
 of the Internet (b) in which the size of the data chunks was variable.
\begin_inset CommandInset label
LatexCommand label
name "fig:chunkSize"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
For the chunk size experiments we conducted two types of experiments, one
 in which the nodes are in a double rack data center and one in which nodes
 are randomly distributed throughout the mapped portion of the Internet.
 For both types we used 50 node topologies which we held constant for all
 experiments.
 Results for these experiments are presented in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:chunkSize"

\end_inset

.
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename results/results_mapperReducer_shaded.png
	width 8.7cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Results for a 50 node topology randomly distributed across the mapped portion
 of the Internet in which the number of map and reduce slots available on
 each host was variable.
\begin_inset CommandInset label
LatexCommand label
name "fig:mapperReducer"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
For experiments in which we varied the number of mapper and reducer slots
 per node we used a chunk size of 6 MB and a 50 node topology randomly distribut
ed across the mapped portion of the Internet.
 The topology was held constant as we varied the number of mapper and reducer
 slots.
 The results of this experiment is presented in Figure 

\begin_inset CommandInset ref
LatexCommand ref
reference "fig:mapperReducer"

\end_inset


\lang english
.
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename results/results_dataCenterNumHosts.png
	width 8.7cm

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
Results for double rack data center topologies in which the total number
 of nodes was variable.
\begin_inset CommandInset label
LatexCommand label
name "fig:dataCenterNumHosts"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename results/results_numHosts.png
	width 8.7cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Results for topologies in which each consecutive point represents the running
 time when 10 additional hosts are randomly added to the previous topology.
 Hosts are distributed across the entire mapped portion of the Internet.
\begin_inset CommandInset label
LatexCommand label
name "fig:numHosts"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename results/results_numHosts_oneAS.png
	width 8.7cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
Results for topologies in which each consecutive point represents the running
 time when 10 additional hosts are randomly added to the previous topology.
 Hosts are distributed across AS 11537 which spans the entire United States
 and parts of southern Canada.
\begin_inset CommandInset label
LatexCommand label
name "fig:numHosts_oneAS"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
We ran three types of experiments that varied the number of nodes.
 We varied the number of nodes in a double rack data center, distributed
 across the mapped portion of the Internet, and distributed across one autonomou
s system consisting of 15 mapped routers which span the United States.
 Each consecutive experiment for the edge based data kept the previous experimen
t's topology and added 10 additional nodes randomly distributed throughout
 the topology.
 The results for the data center and edge based experiments are presented
 in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:dataCenterNumHosts"

\end_inset

, Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:numHosts"

\end_inset

 and Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:numHosts_oneAS"

\end_inset

.
\end_layout

\begin_layout Section
Conclusions
\begin_inset CommandInset label
LatexCommand label
name "sec:Conclusions"

\end_inset


\end_layout

\begin_layout Standard
From our results we can conclude that the data chunk size used in edge based
 MapReduce jobs should be considerably smaller than what is usually used
 in data center MapReduce jobs.
 Figure 
\backslash
fig:chunkSize shows that while certain data center topologies can suffer
 if the chunk size is too small the edge based network produces the best
 results when the chunk size is small.
 
\end_layout

\begin_layout Standard
We can see from Figure 
\backslash
fig:dataCenterNumHosts that the total run time increases as more nodes are
 added after a certain point for the data center topology evaluated.
 This is probably caused because the sort algorithm becomes I/O bound and
 no longer benefits from extra computational power.
 It is more difficult to draw conclusions about the effects of adding more
 nodes in an edge based cloud.
 Perhaps the only definite conclusion that we can draw is that the default
 scheduler should be replaced by one that is aware of the network topology
 and node specifications.
 In our evaluation it was possible for the scheduler to do very well with
 one topology and very poorly when a distant node is added which the scheduler
 blindly scheduled tasks to.
 The development of this new scheduler is left for future work.
\end_layout

\begin_layout Section
Future Research
\begin_inset CommandInset label
LatexCommand label
name "sec:Future-Research"

\end_inset


\end_layout

\begin_layout Standard
We did not present results related to modifications to the Hadoop scheduling
 algorithm used by MRPerf.
 This is primarily because the early release of MRPerf we used for our simulatio
ns does not support realistic node failures.
 We are confident that a scheduling algorithm that is aware of the round
 trip times to each node and heterogeneity of the nodes would perform better
 than the default Hadoop scheduling algorithm.
 An analysis that involves the simulation of node churn is also left for
 future work.
 
\end_layout

\begin_layout Standard
A possible alternate simulation tool to MRPerf is CloudSim
\backslash
cite:CloudSim.
 Future research could investigate the use of MapReduce with this Java based
 simulation tool that has recently been developed.
 CloudSim was built on top of the existing tools, JavaSim and GridSim, which
 have already proven themselves valid for simulation.
 CloudSim itself is an effort to add a new layer of features on top of these
 tools that are more critical and immediately relevant to cloud computing.
 By abstracting away all the lower levels CloudSim can focus on doing its
 job correctly and at the same time trust that the layers below are functioning
 correctly as well.
 The results gathered from simulation with this tool would be a valuable
 comparison to the results we present in this paper.
 Other avenues for research could address the security and privacy implications
 for this type of computing paradigm.
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
bibliographystyle{acm}
\end_layout

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
nocite{*}
\end_layout

\end_inset

 
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintAll"
bibfiles "finalreport"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
